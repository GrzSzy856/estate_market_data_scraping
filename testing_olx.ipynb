{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scrapy\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OlxScraper:\n",
    "    \"\"\"\n",
    "    A class that allows the scraping of data from the \"olx\" website, which provides information about housing offers in Poland.\n",
    "\n",
    "    Attributes:\n",
    "        cities_individual_urls (dict): Dictionary containing individual offers utl for given cities in a list. (Set internally)\n",
    "\n",
    "    Methods:\n",
    "        check_page_exists():\n",
    "            Checks whether given page contains any offers. (Only for internal purposes)\n",
    "        get_all_urls():\n",
    "            Generates all individual offers url's based on the main url's.\n",
    "        get_json():\n",
    "            Scrapes json content from html code.\n",
    "        scrap_data():\n",
    "            Scraps data from the individual offers pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _base_url = 'https://www.olx.pl/nieruchomosci/mieszkania/sprzedaz/'\n",
    "    _params = '/?page={f}&view=grid'\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the scraper.\"\"\"\n",
    "\n",
    "        self.cities_individual_urls = {\n",
    "            'Katowice': [],\n",
    "            'Krakow': [],\n",
    "            'Warszawa': [],\n",
    "            'Wroclaw': []\n",
    "        }\n",
    "\n",
    "    def check_page_exists(self, url):\n",
    "        \"\"\"Checks whether given page exists. (Only for internal purposes)\"\"\"\n",
    "\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if not soup.find('p', string='Sprawdź ogłoszenia w większej odległości:') and (response.url == url or re.search(r'page=(\\d+)', url).group(1) == '1'):\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "\n",
    "    def get_all_urls(self, print_page_numbers=False):\n",
    "        \"\"\"\n",
    "        Generates all individual offers url's.\n",
    "        \n",
    "        Args:\n",
    "            print_page_numbers (bool, optional): Indicates whether to print city names and page numbers, while scraping the data.\n",
    "\n",
    "        Returns:\n",
    "            cities_pages (dict): Dictionary with url's of available pages for each city.\n",
    "        \"\"\"\n",
    "\n",
    "        for city_name in self.cities_individual_urls:\n",
    "            page_number = 1\n",
    "            hrefs = []\n",
    "            while True:\n",
    "                full_url = ''.join([OlxScraper._base_url, city_name, OlxScraper._params]).format(f = page_number)\n",
    "                if not self.check_page_exists(full_url):\n",
    "                    print(f\"Page: {full_url} doesn't exist\")\n",
    "                    break\n",
    "\n",
    "                if print_page_numbers:\n",
    "                    print(f\"{city_name}: Page {page_number} exists.\")\n",
    "\n",
    "                response = requests.get(full_url, headers=headers, allow_redirects=True)\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                a_elements = soup.find_all('a', {\"class\": \"css-z3gu2d\"})\n",
    "\n",
    "                for a in a_elements:\n",
    "                    href = a['href']\n",
    "                    if 'otodom' not in href:\n",
    "                        href = 'https://www.olx.pl' + href\n",
    "                        hrefs.append(href)\n",
    "\n",
    "                page_number += 1\n",
    "            self.cities_individual_urls[city_name].extend(hrefs)\n",
    "\n",
    "    \n",
    "    def get_json(self, url):\n",
    "        \"\"\"\n",
    "        Scraps json content from html code.\n",
    "\n",
    "        Args:\n",
    "            url (str): url to an olx offer page.\n",
    "        Returns:\n",
    "            json_content (json): Data related to an offer.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        soup_str = str(soup)\n",
    "\n",
    "        start_marker = '__PRERENDERED_STATE__= \"'\n",
    "        end_marker = 'window.__TAURUS__='\n",
    "\n",
    "        start_index = soup_str.find(start_marker) + len(start_marker)\n",
    "\n",
    "        end_index = soup_str.find('\";', start_index)\n",
    "\n",
    "        if end_index != -1:\n",
    "            result = soup_str[start_index:end_index]\n",
    "        else:\n",
    "            result = soup_str[start_index:]\n",
    "\n",
    "        clean_string = result.replace('\\\\\"', '\"').replace('\\\\\\\\\"', '\\\\\"')\n",
    "\n",
    "        json_content = json.loads(clean_string)['ad']['ad']\n",
    "        return(json_content)\n",
    "\n",
    "\n",
    "    def scrap_data(self, print_page_numbers=False):\n",
    "        \"\"\"\n",
    "        Scraps data from the json.\n",
    "        \n",
    "        Args:\n",
    "            print_page_numbers (bool, optional): Indicates whether to print city names and page numbers, while scraping the data.\n",
    "\n",
    "        Returns:\n",
    "            olxData (Data Frame): a table with data scraped from all the offer pages.\n",
    "        \"\"\"\n",
    "        \n",
    "        olxData = pd.DataFrame()\n",
    "        for city_name, offer_urls in self.cities_individual_urls.items():\n",
    "            for i, offer_url in enumerate(offer_urls):\n",
    "                try:\n",
    "                    json_content = self.get_json(offer_url)\n",
    "                except Exception as E:\n",
    "                    print('No data has been found in the json file')\n",
    "                    break\n",
    "                \n",
    "                generalInformation = {\n",
    "                'id': json_content['id'],\n",
    "                'source': 'OLX',\n",
    "                'date': date.today().strftime('%Y-%m-%d'),\n",
    "                'city_name': city_name,\n",
    "                'market_type': 'Prywatny',\n",
    "                'create_date': json_content['createdTime'],\n",
    "                'modify_date': json_content['lastRefreshTime'],\n",
    "                'title': json_content['title'],\n",
    "                'url': json_content['url'],\n",
    "                'price': json_content['price']['regularPrice']['value']\n",
    "\n",
    "                }\n",
    "\n",
    "                params = {}\n",
    "                params.setdefault('price_per_m', None)\n",
    "                params.setdefault('floor', None)\n",
    "                params.setdefault('furniture', None)\n",
    "                params.setdefault('market_type', None)\n",
    "                params.setdefault('area', None)\n",
    "                params.setdefault('rooms_num', None)\n",
    "\n",
    "                for data in json_content['params']:\n",
    "                    label = data['key']\n",
    "                    values = data['normalizedValue']\n",
    "\n",
    "                    if (label == 'price_per_m') & (len(values) > 0):\n",
    "                        params['price_per_m'] = values\n",
    "\n",
    "                    if (label == 'floor_select') & (len(values) > 0):\n",
    "                        params['floor'] = values\n",
    "\n",
    "                    if (label == 'furniture') & (len(values) > 0):\n",
    "                        params['furniture'] = values\n",
    "\n",
    "                    if (label == 'market') & (len(values) > 0):\n",
    "                        params['market_type'] = values\n",
    "\n",
    "                    if (label == 'm') & (len(values) > 0):\n",
    "                        params['area'] = values\n",
    "\n",
    "                    if (label == 'rooms') & (len(values) > 0):\n",
    "                        params['rooms_num'] = values\n",
    "\n",
    "                allInformation = generalInformation | params\n",
    "                allInformationDF = pd.DataFrame(allInformation, index=[i])\n",
    "                olxData = pd.concat([olxData, allInformationDF])\n",
    "\n",
    "                if print_page_numbers:\n",
    "                    print(f\"City: {city_name}, page number: {i} out of {len(offer_urls)}\")\n",
    "        \n",
    "        return olxData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
