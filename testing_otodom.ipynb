{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scrapy\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OtodomScraper:\n",
    "    \"\"\"\n",
    "    A class that allows the scraping of data from the \"otodom\" website, which provides information about housing offers in Poland.\n",
    "\n",
    "    Attributes:\n",
    "        key (str): The otodom url key that enables the data scraping.\n",
    "        cities_pages (dict): Dictionary containing the number of available pages for given cities. (Set internally)\n",
    "        cities_individual_urls (dict): Dictionary containing individual offers utl for given cities in a list. (Set internally)\n",
    "\n",
    "    Methods:\n",
    "        check_page_exists():\n",
    "            Checks whether given page contains any offers. (Only for internal purposes)\n",
    "        get_individual_urls():\n",
    "            Scraps the url's of all the offer listings. (Only for internal purposes)\n",
    "        get_all_urls():\n",
    "            Generates all individual offers url's based on the main url's.\n",
    "        scrap_data():\n",
    "            Scraps data from the individual offers pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _base_url = 'https://www.otodom.pl/pl/wyniki/sprzedaz/mieszkanie'\n",
    "    _params = '?limit=72&viewType=listing&page='\n",
    "    _cities = {\n",
    "            'Katowice': '/slaskie/katowice/katowice/katowice', \n",
    "            'Kraków': '/malopolskie/krakow/krakow/krakow', \n",
    "            'Warszawa': '/mazowieckie/warszawa/warszawa/warszawa',\n",
    "            'Wrocław': '/dolnoslaskie/wroclaw/wroclaw/wroclaw'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        \"\"\"\n",
    "        Initializes the scraper with a given key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The otodom url key that enables the data scraping.        \n",
    "        \"\"\"\n",
    "\n",
    "        self.key = key\n",
    "        self.cities_pages = {\n",
    "            'Katowice': 0,\n",
    "            'Kraków': 0,\n",
    "            'Warszawa': 0,\n",
    "            'Wrocław': 0\n",
    "        }\n",
    "        self.cities_individual_urls = {\n",
    "            'Katowice': [],\n",
    "            'Kraków': [],\n",
    "            'Warszawa': [],\n",
    "            'Wrocław': []\n",
    "        }\n",
    "\n",
    "\n",
    "    def check_page_exists(self, url):\n",
    "        \"\"\"Checks whether given page exists. (Only for internal purposes)\"\"\"\n",
    "\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            json_content = BeautifulSoup(response.text, 'html.parser')\n",
    "            if not json_content.find('h3', string='Nie znaleźliśmy żadnych ogłoszeń'):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_individual_urls(self, city_name, main_url):\n",
    "        \"\"\"Scraps the url's of all the offer listings.\"\"\"\n",
    "\n",
    "        response = requests.get(main_url, headers=headers)\n",
    "        json_content = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        hrefs = []\n",
    "        for section in json_content.find_all('section'):\n",
    "            classes = section.get('class')\n",
    "            href = section.find_all('a', {\"class\": \"css-16vl3c1 e17g0c820\"})\n",
    "            if classes:\n",
    "                classes = [classes for c in classes if 'eeungyz1' in c]\n",
    "                if len(classes) > 0:\n",
    "                    for h in href:\n",
    "                        link = f\"https://www.otodom.pl/_next/data/{self.key}{h.get('href')}.json\"\n",
    "                        hrefs.append(link)\n",
    "        self.cities_individual_urls[city_name].extend(hrefs)\n",
    "        \n",
    "\n",
    "    def get_all_urls(self, print_page_numbers=False):\n",
    "        \"\"\"\n",
    "        Generates all individual offers url's based on the main url's.\n",
    "        \n",
    "        Args:\n",
    "            print_page_numbers (bool, optional): Indicates whether to print city names and page numbers, while scraping the data.\n",
    "\n",
    "        Returns:\n",
    "            cities_pages (dict): Dictionary with number of available pages for each city.\n",
    "        \"\"\"\n",
    "\n",
    "        for city_name, city_url in OtodomScraper._cities.items():\n",
    "            url = ''.join([OtodomScraper._base_url, city_url, OtodomScraper._params])\n",
    "            page_number = 1 \n",
    "            while True:\n",
    "                full_url = url + str(page_number)\n",
    "                if not self.check_page_exists(full_url):\n",
    "                    print(f\"Page: {full_url} doesn't exist\")\n",
    "                    break\n",
    "\n",
    "                if print_page_numbers:\n",
    "                    print(f\"{city_name}: Page {page_number} exists.\")\n",
    "\n",
    "                self.get_individual_urls(city_name, full_url)\n",
    "                page_number += 1\n",
    "                self.cities_pages[city_name] += 1\n",
    "        \n",
    "        return self.cities_pages\n",
    "    \n",
    "\n",
    "    def scrap_data(self, print_page_numbers=False):\n",
    "        \"\"\"\n",
    "        Scraps data from the individual offers pages.\n",
    "        \n",
    "        Args:\n",
    "            print_page_numbers (bool, optional): Indicates whether to print city names and page numbers, while scraping the data.\n",
    "\n",
    "        Returns:\n",
    "            otoDomData (Data Frame): a table with data scraped from all the offer pages.\n",
    "        \"\"\"\n",
    "        \n",
    "        otoDomData = pd.DataFrame()\n",
    "        for city_name, offer_urls in self.cities_individual_urls.items():\n",
    "            for i, offer_url in enumerate(offer_urls):\n",
    "                response = requests.get(offer_url, headers=headers)\n",
    "                try:\n",
    "                    json_content = json.loads(response.content)['pageProps']['ad']\n",
    "                except KeyError:\n",
    "                    print('No data has been found in the json file')\n",
    "                    break\n",
    "                print(offer_url)\n",
    "\n",
    "                generalInformation = {\n",
    "                'id': json_content.get('id', None),\n",
    "                'source':'OtoDom',\n",
    "                'date': date.today().strftime('%Y-%m-%d'),\n",
    "                'city': city_name,\n",
    "                'market_type': json_content.get('market', None),\n",
    "                'create_date': json_content.get('createdAt', None),\n",
    "                'modify_date': json_content.get('modifiedAt', None),\n",
    "                'title': json_content.get('title', None),\n",
    "                'url': json_content.get('url', None),\n",
    "                'price': json_content.get('target', {}).get('Price', None),\n",
    "                'price_per_m': json_content.get('target', {}).get('Price_per_m', None),\n",
    "                'area': json_content.get('target', {}).get('Area', None),\n",
    "                'building_year': json_content.get('target', {}).get('Build_year', None),\n",
    "                'construction_status': json_content.get('target', {}).get('Construction_status', None)\n",
    "                }\n",
    "\n",
    "\n",
    "                topInformation = {}\n",
    "                topInformation.setdefault('rooms_num', None)\n",
    "                topInformation.setdefault('car', None)\n",
    "                topInformation.setdefault('rent', None)\n",
    "                topInformation.setdefault('floor', None)\n",
    "                topInformation.setdefault('outdoor', None)\n",
    "                topInformation.setdefault('heating', None)\n",
    "\n",
    "                for data in json_content['topInformation']:\n",
    "                    label = data['label']\n",
    "                    values = data['values']\n",
    "\n",
    "                    if (label == 'rooms_num') & (len(values) > 0):\n",
    "                        topInformation['rooms_num'] = values[0]\n",
    "\n",
    "                    if (label == 'car') & (len(values) > 0):\n",
    "                        topInformation['car'] = values[0]\n",
    "\n",
    "                    if (label == 'rent') & (len(values) > 0):\n",
    "                        topInformation['rent'] = values[0]\n",
    "\n",
    "                    if (label == 'floor') & (len(values) > 0):\n",
    "                        topInformation['floor'] = values[0]\n",
    "\n",
    "                    if (label == 'outdoor') & (len(values) > 0):\n",
    "                        topInformation['outdoor'] = values[0]\n",
    "\n",
    "                    if (label == 'heating') & (len(values) > 0):\n",
    "                        topInformation['heating'] = values[0]\n",
    "\n",
    "\n",
    "                additionalInformation = {}\n",
    "                additionalInformation.setdefault('building_material', None)\n",
    "                additionalInformation.setdefault('windows_type', None)\n",
    "                additionalInformation.setdefault('media_types', None)\n",
    "                additionalInformation.setdefault('security_types', None)\n",
    "                additionalInformation.setdefault('lift', None)\n",
    "\n",
    "                for data in json_content['additionalInformation']:\n",
    "                    label = data['label']\n",
    "                    values = data['values']\n",
    "\n",
    "                    if (label == 'building_material') & (len(values) > 0):\n",
    "                        additionalInformation['building_material'] = values[0]\n",
    "\n",
    "                    if (label == 'windows_type') & (len(values) > 0):\n",
    "                        additionalInformation['windows_type'] = values[0]\n",
    "\n",
    "                    if (label == 'media_types') & (len(values) > 0):\n",
    "                        additionalInformation['media_types'] = values[0]\n",
    "\n",
    "                    if (label == 'security_types') & (len(values) > 0):\n",
    "                        additionalInformation['security_types'] = values[0]\n",
    "\n",
    "                    if (label == 'lift') & (len(values) > 0):\n",
    "                        additionalInformation['lift'] = values[0]\n",
    "\n",
    "\n",
    "                allInformation = generalInformation | additionalInformation | topInformation\n",
    "                allInformationDF = pd.DataFrame(allInformation, index=[i])\n",
    "                otoDomData = pd.concat([otoDomData, allInformationDF])\n",
    "\n",
    "                if print_page_numbers:\n",
    "                    print(f\"City: {city_name}, page number: {i} out of {len(offer_urls)}\")\n",
    "        \n",
    "        return otoDomData\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
